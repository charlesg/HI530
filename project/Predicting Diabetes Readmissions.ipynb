{"cells":[{"cell_type":"markdown","metadata":{"deepnote_cell_type":"markdown"},"source":["## Predicting Diabetes Readmissions\n","\n","This project will be working with a diabetic dataset to evaluate predicting/explaining factors for diabetic patients' readmissions. A journal article related to this dataset is also attached to give the background about this dataset. For additional information about this dataset, see:\n","\n","https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1619292307660,"source_hash":"bdee9125"},"outputs":[],"source":["import warnings\n","# Load/Review/Visualize\n","from zipfile import ZipFile\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas_profiling\n","# for K-Mean\n","from sklearn.cluster import KMeans\n","from pandas.plotting import parallel_coordinates\n","from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n","# Model/Prediction\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, auc\n","from sklearn.model_selection import train_test_split, cross_val_score \n","from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n","import statsmodels.api as sm \n","from keras_visualizer import visualizer \n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","# DMBA Imports\n","from dmba import regressionSummary, exhaustive_search\n","from dmba import backward_elimination, forward_selection, stepwise_selection\n","from dmba import adjusted_r2_score, AIC_score, BIC_score\n","from dmba import plotDecisionTree, classificationSummary, regressionSummary, gainsChart, liftChart"]},{"cell_type":"code","execution_count":null,"metadata":{"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":9,"execution_start":1619292308892,"source_hash":"cc5718a5"},"outputs":[],"source":["# Inputs\n","datafile=\"dataset_diabetes.zip\""]},{"cell_type":"code","execution_count":null,"metadata":{"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2364,"execution_start":1619292751838,"scrolled":true,"source_hash":"7fa792d"},"outputs":[],"source":["mapping=[]\n","diabetic=[]\n","\n","with ZipFile(datafile) as rawData:\n","        for info in rawData.infolist():\n","            print(\"Reading: \", info.filename)\n","            with rawData.open(info.filename) as f:\n","                if 'IDs_mapping' in info.filename:\n","                    mapping = pd.read_csv(f)\n","                else:\n","                    diabetic = pd.read_csv(f)\n","\n","# Removing Colums with too many unknow values or lack of relevance based on description\n","dropped_Cols=['weight', 'medical_specialty', 'payer_code', 'encounter_id', 'patient_nbr']\n","diabetic.drop(columns=dropped_Cols, inplace=True)\n","# Drop remaining rows with unknown values\n","diabetic.drop(index=(diabetic[diabetic.isin(['?']).any(axis=1)].index), inplace=True)\n","# Let's check what's left\n","diabetic.shape\n","diabetic.head(20)\n","\n","#diabetic = pd.get_dummies(diabetic, prefix_sep='_', drop_first=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The profile is completely optionional and not required to run the models\n","# It was used to review the dataset. It's a useful tool to look at corelations and review\n","# how the data should be processed.\n","\n","#profile1 = pandas_profiling.ProfileReport(diabetic)\n","#profile1.to_file('profile1.html')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This can be used to create a widget that profiles the data.\n","#profile1.to_widgets()"]},{"cell_type":"markdown","metadata":{},"source":["## Categorical Data\n","\n","### Gender\n","\n","We normalize this feature by creating a single \"isFemale\" binary feature and we remove the one patient of unknown gender. We can than drop the gender."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetic[\"isFemale\"] = False\n","diabetic.loc[diabetic.gender == \"Female\", \"isFemale\"] = True\n","diabetic = diabetic[diabetic.gender != \"Unknown/Invalid\"]\n","diabetic.drop(columns=['gender'], inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Race\n","\n","With race, I will simply create categorical binary features, and we can drop the gender.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetic = pd.get_dummies(data = diabetic, columns = [\"race\"], prefix = \"race\", drop_first=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Diagnostic\n","\n","There are three independant diagnostics and each can be over 700 different values. For this purpose, I will simplify the diagnostic into a few relevant categories.\n","\n","A few of the diagnostics start with E and with V. Those will be other. For numeric icd9 codes, I followed information from the paper going with the data:\n","\n","> The following abbreviations are used for particular icd9 codes: “circulatory” for icd9: 390–459, 785, “digestive” for icd9: 520–579, 787, “genitourinary” for icd9: 580–629, 788, “diabetes” for icd9: 250.xx, “injury” for icd9: 800–999, “musculoskeletal” for icd9: 710–739, “neoplasms” for icd9: 140–239, “respiratory’’ for icd9: 460–519, 786, and “other” for otherwise.\n","\n","From: https://www.hindawi.com/journals/bmri/2014/781670/#results-and-discussion\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def fix_diag(diabetic, icd9) :\n","    if diabetic[icd9][0] == \"E\" :\n","        return \"Other\"\n","    elif diabetic[icd9][0] == \"V\" :\n","        return \"Other\"\n","    else :\n","        num = float(diabetic[icd9])\n","        \n","        if np.trunc(num) == 250 :\n","            return \"diabetes\"\n","        elif num <= 139 :\n","            return \"other\"\n","        elif num <= 279 :\n","            return \"neoplasms\"\n","        elif num <= 389 :\n","            return \"other\"\n","        elif num <= 459 :\n","            return \"circulatory\"\n","        elif num <= 519 :\n","            return \"respiratory\"\n","        elif num <= 579 :\n","            return \"digestive\"\n","        elif num <= 629 :\n","            return \"genitourinary\"\n","        elif num <= 679 :\n","            return \"other\"\n","        elif num <= 709 :\n","            return \"neoplasms\"\n","        elif num <= 739 :\n","            return \"musculoskeletal\"\n","        elif num <= 759 :\n","            return \"other\"\n","        elif num in [780, 781, 782, 783, 784] : \n","            return \"neoplasms\"\n","        elif num == 785 :\n","            return \"circulatory\"\n","        elif num == 786 :\n","            return \"respiratory\"\n","        elif num == 787 :\n","            return \"digestive\"\n","        elif num == 788 :\n","            return \"genitourinary\"\n","        elif num == 789 :\n","            return \"digestive\"\n","        elif num in np.arange(790, 800) :\n","            return \"neoplasms\"\n","        elif num >= 800 :\n","            return \"injury\"\n","        else :\n","            return num\n","\n","diabetic[\"diag1_norm\"] = diabetic.apply(fix_diag, axis=1, icd9=\"diag_1\")\n","diabetic[\"diag2_norm\"] = diabetic.apply(fix_diag, axis=1, icd9=\"diag_2\")\n","diabetic[\"diag3_norm\"] = diabetic.apply(fix_diag, axis=1, icd9=\"diag_3\")"]},{"cell_type":"markdown","metadata":{},"source":["Armed with a \"textual\" diagnosis instead of a code, we can now create binary variables, once for each diagnosis.\n","- diabetes_diagnosis\n","- other_diagnosis\n","- etc."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def diagnose (diabetic, diag) :\n","    if (diabetic[\"diag1_norm\"] == diag) | (diabetic[\"diag2_norm\"] == diag) | (diabetic[\"diag3_norm\"] == diag) :\n","        return True\n","    else :\n","        return False\n","\n","for val in ['diabetes', 'other', 'circulatory', 'neoplasms', 'respiratory', 'injury', 'musculoskeletal', 'digestive', 'genitourinary'] :\n","    name = val + \"_diagnosis\"\n","    diabetic[name] = diabetic.apply(diagnose, axis = 1, diag=val)\n","\n","dropped_Cols=['diag1_norm', 'diag2_norm', 'diag3_norm', 'diag_1', 'diag_2', 'diag_3']\n","diabetic.drop(columns=dropped_Cols, inplace=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Drugs\n","\n","- We'll remove drugs rarely present like `examide`, `citoglipton` and `metformin-rosiglitazone`\n","- For other \"common\" drugs found more often, we can create a binary variable where No is FALSE and Down, Steady or Up means the drug was \"present\" so TRUE."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["common_drugs = ['metformin', 'repaglinide', 'glimepiride', 'glipizide',\n","                'glyburide', 'pioglitazone', 'rosiglitazone', 'insulin']\n","rare_drugs = [\"nateglinide\", \"chlorpropamide\", \"acetohexamide\", \"tolbutamide\",\n","             \"acarbose\", \"miglitol\", \"troglitazone\", \"tolazamide\", \"examide\",\n","             \"citoglipton\", \"glyburide-metformin\", \"glipizide-metformin\",\n","             \"glimepiride-pioglitazone\", \"metformin-rosiglitazone\", \"metformin-pioglitazone\"]\n","\n","drugs = common_drugs + rare_drugs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for drug in common_drugs :\n","    name = \"take_\" + drug\n","    diabetic[name] = diabetic[drug].isin([\"Down\", \"Steady\", \"Up\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetic.drop(columns=drugs, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Readmissions (our outcome)\n","\n","The current variable `readmitted` is `NO`, `>30` if readmitted after 30 days, or `<30` if readmitted under 30 days.\n","I will create a single dummy \"readmitted\" which means the patient was readmitted under 30 days, the outcome we are looking to predict."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetic = pd.get_dummies(data = diabetic, columns = [\"readmitted\"], prefix = \"readmit\", drop_first=False)\n","# I didn't use drop_first to be sure I was dropping readmit_NO\n","diabetic = diabetic.drop([\"readmit_NO\"], axis = 1)"]},{"cell_type":"markdown","metadata":{},"source":["After consideration, I will drop readmit_>30 and entirely focus on patients readmitted under 30 days for this study."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetic = diabetic.drop([\"readmit_>30\"], axis = 1)\n","diabetic.rename(columns = {'readmit_<30': 'readmitted'}, inplace = True)"]},{"cell_type":"markdown","metadata":{},"source":["### A1C and GluSerum tests\n","\n","According to the paper, `A1C` of `>7` or `>8` would be considered abnormal. Same for `>200` or `>300` for the GluSerum test. I will combine these two to reduce the number of categorical variables. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetic[\"A1C\"] = diabetic[\"A1Cresult\"]\n","diabetic.loc[diabetic.A1Cresult.isin([\">7\", \">8\"]), \"A1C\"] = \"Abnorm\"\n","\n","diabetic = pd.get_dummies(data = diabetic, columns = [\"A1C\"], prefix = \"A1C\", drop_first=False)\n","diabetic = diabetic.drop([\"A1Cresult\", \"A1C_None\"], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetic[\"glu_serum\"] = diabetic[\"max_glu_serum\"]\n","diabetic.loc[diabetic.max_glu_serum.isin([\">200\", \">300\"]), \"glu_serum\"] = \"Abnorm\"\n","\n","diabetic = pd.get_dummies(data = diabetic, columns = [\"glu_serum\"], prefix = \"glu_serum\", drop_first=False)\n","diabetic = diabetic.drop([\"max_glu_serum\", \"glu_serum_None\"], axis = 1)"]},{"cell_type":"markdown","metadata":{},"source":["### Change and diabetesMed\n","\n","These two features simply need to converted:\n","- for `Change`, from `Ch` to `True` and `No` to `False`\n","- for `diabetesMed`, from `Yes` to `True` and `No` to `False`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetic.loc[diabetic.change == \"Ch\", \"change\"] = True\n","diabetic.loc[diabetic.change == \"No\", \"change\"] = False\n","\n","diabetic.loc[diabetic.diabetesMed == \"Yes\", \"diabetesMed\"] = True\n","diabetic.loc[diabetic.diabetesMed == \"No\", \"diabetesMed\"] = False"]},{"cell_type":"markdown","metadata":{},"source":["## Unmapped Data\n","\n","After, I elected to remove columns I was not clear would yield useful information for this model\n","- admission_type_id\n","- discharge_disposition_id\t\n","- admission_source_id\n","\n","I could no easily find a mapping for these IDs and give the information meaning."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetic.drop(columns=['admission_type_id', 'discharge_disposition_id', 'admission_source_id'], inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Quantitative Variables\n","\n","### Age\n","\n","Age should be quantitative, we will return it to a numerical value by simply taking the first digit for the decade:\n","- 10-20 == 10\n","- 20-30 == 20\n","etc."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def cutage (df) :\n","    return int(df.age[-4::].replace('-', '').replace(')', ''))\n","\n","diabetic[\"decade\"] = diabetic.apply(cutage, axis = 1)\n","#var_quanti.append(\"age_num\")\n","diabetic = diabetic.drop(\"age\", axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["quant_values = ['decade', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', \n","                'num_medications', 'number_outpatient', 'number_emergency', \n","                'number_inpatient','number_diagnoses' ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetic"]},{"cell_type":"markdown","metadata":{},"source":["## Re-profiling for a quick verification\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The profile is completely optionional and not required to run the models\n","# It was used to review the dataset.\n","profile2 = pandas_profiling.ProfileReport(diabetic)\n","profile2.to_file('profile2.html')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["profile2.to_widgets()"]},{"cell_type":"markdown","metadata":{},"source":["## Correlations\n","\n","Reviewing correlations is a useful exercise. Some of this was done twice already, using the pandas-profiling function. Clear correlation maps were produced, before AND after the data was cleaned up and organized.\n","\n","Following the organization, we have dropped the number of warnings to only 6 obvious ones. And the correlation maps makes more sense (see below).\n","\n","We have definite stronger correlations between: time_in_hospital, num_lab_procedures, num_procedures and number of medications. This cluster of correlations makes sense.\n","\n","From a \"readmission\" perspective, we do notice some correlations with the num_emergency and num_inpatiet, also expected.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":536,"execution_start":1619292729056,"source_hash":"d8fd7ee4"},"outputs":[],"source":["corr = diabetic.corr()\n","sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)"]},{"cell_type":"markdown","metadata":{"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"d68ace47"},"source":["# 2. Visualize your dataset.\n","\n","We will consider the following visualization technique:\n","- Basic charts (e.g., bar, line, ad scatter plots)*\n","- Heatmaps*\n","- Multidimensional visualization\n","- Network\n","- Treemaps"]},{"cell_type":"markdown","metadata":{"deepnote_cell_type":"code","tags":[]},"source":["## Quant Data\n","\n","First, let quickly vizualize the quantitative data:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We will be using the list of quant_values\n","# We need to count how many rows we need\n","rowsTot = -(-(len(quant_values)) // 3)\n","\n","# Where we start\n","rowNum=0\n","colNum=0\n","fig, axs = plt.subplots(rowsTot, 3, figsize = (20, 20))\n","\n","for feature in quant_values:\n","    sns.countplot(data = diabetic, y = feature, ax = axs[rowNum, colNum])\n","    axs[rowNum, colNum].set_title(feature + str(rowNum) + str(colNum))\n","    # Col numbers flips between 1 and 2, Rows increase by one\n","    if colNum < 2: \n","        colNum += 1\n","    else:\n","        colNum = 0\n","        rowNum += 1\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Distribution of all readmission\n","\n","Next we look at the distribution of our outcome, patients readmitted under 30 days."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.countplot(data = diabetic, y = 'readmitted').set_title('Distribution of Readmission')"]},{"cell_type":"markdown","metadata":{},"source":["### Time in Hospital\n","\n","We can look at some of our features and compare them for the patients readmitted, versus those not. In this case, we look at time in hospital."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(13,7),)\n","ax=sns.kdeplot(diabetic.loc[(diabetic['readmitted'] == 0),'time_in_hospital'] , color='b',shade=True,label='Not Readmitted')\n","ax=sns.kdeplot(diabetic.loc[(diabetic['readmitted'] == 1),'time_in_hospital'] , color='r',shade=True, label='Readmitted')\n","ax.set(xlabel='Time in Hospital', ylabel='Frequency')\n","plt.legend()\n","plt.title('Time in Hospital compared to readmission')"]},{"cell_type":"markdown","metadata":{},"source":["# 3. K-mean\n","\n","Next we look to apply clustering algorithms:\n","- first apply K-mean to evaluate the clusters\n","- next apply at least one of the hierarchical clustering"]},{"cell_type":"markdown","metadata":{},"source":["## Cluster Sizes\n","\n","First we evaluate different cluster sizes to see how we want to cluster the data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# k-means clustering\n","inertia = []\n","for n_clusters in range(1, 10):\n","    kmeans = KMeans(n_clusters=n_clusters, random_state=1).fit(diabetic)\n","    inertia.append(kmeans.inertia_ / n_clusters)\n","inertias = pd.DataFrame({'n_clusters': range(1, 10), 'inertia': inertia})\n","ax = inertias.plot(x='n_clusters', y='inertia')\n","plt.xlabel('Number of clusters(k)')\n","plt.ylabel('Average Within-Cluster Squared Distances')\n","plt.ylim((0, 1.1 * inertias.inertia.max()))\n","ax.legend().set_visible(False)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["From reviewing the plot above, we can see that anything 4 clusters and above will have similar results. So we stick with `n=4` for the next steps."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kmeans = KMeans(n_clusters=4, random_state=0).fit(diabetic)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Plot of centroid\n","centroids = pd.DataFrame(kmeans.cluster_centers_, columns=diabetic.columns)\n","centroids['cluster'] = ['Cluster {}'.format(i+1) for i in centroids.index]\n","\n","fig = plt.figure(figsize=(10,6))\n","fig.subplots_adjust(right=3)\n","ax = parallel_coordinates(centroids, class_column='cluster', colormap='Dark2', linewidth=5)\n","plt.legend(loc='center left', bbox_to_anchor=(0.95, 0.5))\n","plt.xlim(-0.5,7.5)\n","centroids"]},{"cell_type":"markdown","metadata":{},"source":["### Hiarchical Clustering\n","\n","From here we start linking clusters, starting from 4."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Z = linkage(diabetic, method='average')\n","\n","fig = plt.figure(figsize=(10, 6))\n","fig.subplots_adjust(bottom=0.23)\n","plt.title('Hierarchical Clustering Dendrogram (Average linkage)')\n","plt.xlabel('Patients')\n","dendrogram(Z, labels=list(diabetic.index), color_threshold=3.9)\n","plt.axhline(y=3.6, color='black', linewidth=0.5, linestyle='dashed')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["memb = fcluster(linkage(diabetic, 'average'), 4, criterion='maxclust')\n","memb = pd.Series(memb, index=diabetic.index)\n","#for key, item in memb.groupby(memb):\n","#    print(key, ': ', ', '.join(item.index))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetic.index = ['{}: {}'.format(cluster, state) for cluster, state in zip(memb, diabetic.index)]\n","sns.clustermap(diabetic, method='ward', col_cluster=False,  cmap=\"mako_r\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Predict\n","\n","Predict the outcome of interest following standard steps. We have a discrete outcome variable (Readmitted Yes/No) so we use the following steps:\n","- Partition your data into 60% training and 40% validation. Use seed = 1.*\n","- Create models and run prediction using the following methods:\n","    - Logistic regression (with at least one variable selection or shrinkage technique).\n","    - Classification trees\n","    - Artificial neural networks (e.g., ANN, CNN, or RNN)\n","    \n","### SkLearn Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We first partition our data and prepare our input variables\n","X = diabetic.drop(['readmitted'],axis=1)\n","y = diabetic[\"readmitted\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)"]},{"cell_type":"markdown","metadata":{},"source":["Armed with our input variables, we can run our logistic regression model, us it to run predictions on our data and review the accuracy of our model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["logit = LogisticRegression(fit_intercept=True, penalty='l2', max_iter=1000)\n","logit.fit(X_train, y_train)\n","logit_pred = logit.predict(X_test)\n","pd.crosstab(pd.Series(y_test, name = 'Actual'), pd.Series(logit_pred, name = 'Predict'), margins = True)\n","print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_test, logit_pred)))\n","print(\"Precision is {0:.2f}\".format(precision_score(y_test, logit_pred)))\n","print(\"Recall is {0:.2f}\".format(recall_score(y_test, logit_pred)))\n","\n","accuracy_logit = accuracy_score(y_test, logit_pred)\n","precision_logit = precision_score(y_test, logit_pred)\n","recall_logit = recall_score(y_test, logit_pred)\n","summary_logit = classificationSummary(y_test, logit.predict(X_test))\n","\n","logit_proba = logit.predict_proba(X_test)\n","logit_result = pd.DataFrame({'actual': y_test, \n","                                'p(0)': [p[0] for p in logit_proba],\n","                                'p(1)': [p[1] for p in logit_proba],\n","                                'predicted': logit.predict(X_test),\n","                               })\n","logit_result = logit_result.sort_values(by=['p(1)'], ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["### StatsModel Logistic Regression, with variable selection\n","\n","As a second test, I will build a LogisticRegression model, using an approach discovered online. We are going to create a model with all variables, and then remove the variable with the highest p-value. By iterations, we are goind to remove all non-relevant features, until all of our variables have a p-value < 0.05. Function below automates this process."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def reduction_variable_logit(X_train, y_train, showVarToDel=False) :\n","    final_model = False\n","    var_to_del = []\n","\n","    while (final_model == False) :\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            log_reg = sm.Logit(y_train, \n","                               X_train.drop(var_to_del, axis = 1).astype(float)).fit(maxiter = 100, disp = False)\n","        max_pvalue = max(log_reg.pvalues)\n","        if max_pvalue < 0.05 :\n","            final_model = True\n","        else :\n","            varToDel = log_reg.pvalues.index[log_reg.pvalues == max(log_reg.pvalues)].values[0]\n","            if showVarToDel :\n","                print(varToDel + \", p-value = \" + str(max(log_reg.pvalues)))\n","            var_to_del.append(varToDel)\n","    \n","    return log_reg, var_to_del\n","\n","log_reg, var_to_del = reduction_variable_logit(X_train, y_train, False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["var_to_del"]},{"cell_type":"markdown","metadata":{},"source":["We finalize by re-running the SKLearn approach with the reduced variable scope as a verificaton that we've improved the model. This is not really a valid step in any ML analysis workflow but I use it to verify my work and my process."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["newlogit = LogisticRegression(fit_intercept=True, penalty='l2', max_iter=1000)\n","newlogit.fit(X_train.drop(var_to_del, axis = 1), y_train)\n","newlogit_pred = newlogit.predict(X_test.drop(var_to_del, axis = 1))\n","pd.crosstab(pd.Series(y_test, name = 'Actual'), pd.Series(newlogit_pred, name = 'Predict'), margins = True)\n","print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_test, newlogit_pred)))\n","print(\"Precision is {0:.2f}\".format(precision_score(y_test, newlogit_pred)))\n","print(\"Recall is {0:.2f}\".format(recall_score(y_test, newlogit_pred)))\n","\n","accuracy_newlogit = accuracy_score(y_test, newlogit_pred)\n","precision_newlogit = precision_score(y_test, newlogit_pred)\n","recall_newlogit = recall_score(y_test, newlogit_pred)\n","\n","newlogit_proba = newlogit.predict_proba(X_test.drop(var_to_del, axis = 1))\n","newlogit_result = pd.DataFrame({'actual': y_test, \n","                                'p(0)': [p[0] for p in newlogit_proba],\n","                                'p(1)': [p[1] for p in newlogit_proba],\n","                                'predicted': newlogit.predict(X_test.drop(var_to_del, axis = 1)),\n","                               })\n","newlogit_result = newlogit_result.sort_values(by=['p(1)'], ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Classification Tree\n","\n","As specific, we then build and test a *Classification Tree* model and compare results."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#full tree\n","fullClassTree = DecisionTreeClassifier(max_depth=28, criterion = \"entropy\", min_samples_split=10)\n","fullClassTree.fit(X_train, y_train)\n","\n","fullClassTree_pred = fullClassTree.predict(X_test)\n","pd.crosstab(pd.Series(y_test, name = 'Actual'), pd.Series(fullClassTree_pred, name = 'Predict'), margins = True)\n","\n","print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_test, fullClassTree_pred)))\n","print(\"Precision is {0:.2f}\".format(precision_score(y_test, fullClassTree_pred)))\n","print(\"Recall is {0:.2f}\".format(recall_score(y_test, fullClassTree_pred)))\n","\n","accuracy_fulltree = accuracy_score(y_test, fullClassTree_pred)\n","precision_fulltree = precision_score(y_test, fullClassTree_pred)\n","recall_fulltree = recall_score(y_test, fullClassTree_pred)\n","\n","fullClassTree_proba = fullClassTree.predict_proba(X_test)\n","fullClassTree_result = pd.DataFrame({'actual': y_test, \n","                                'p(0)': [p[0] for p in fullClassTree_proba],\n","                                'p(1)': [p[1] for p in fullClassTree_proba],\n","                                'predicted': fullClassTree.predict(X_test),\n","                               })\n","fullClassTree_result = fullClassTree_result.sort_values(by=['p(1)'], ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["### ANN\n","\n","Our third model will use artificial neural networks."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create model\n","annmodel = Sequential()\n","annmodel.add(Dense(12, input_dim=38, activation='relu'))\n","annmodel.add(Dense(8, activation='relu'))\n","annmodel.add(Dense(1, activation='sigmoid'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Compile model\n","annmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#print model summary\n","print('Neural Network Model Summary: ')\n","print(annmodel.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fix int to np.int so they fit in Tensor\n","X_ann=np.asarray(X).astype(np.int)\n","y_ann=np.asarray(y).astype(np.int)\n","X_train_ann=np.asarray(X_test).astype(np.int)\n","y_train_ann=np.asarray(y_test).astype(np.int)\n","X_test_ann=np.asarray(X_test).astype(np.int)\n","y_test_ann=np.asarray(y_test).astype(np.int)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the model; set verbose to 1 or 2 to see the training metrics in each epoch\n","history = annmodel.fit(X_ann, y_ann, validation_split=0.40, epochs=100, batch_size=2000, verbose=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Performance Review\n","\n","Next we judge the performance of various supervised techniques that we applied to the dataset. We look to:\n","- Print the confusion matrix and the accuracy rate for each technique\n","- Plot Gains and Lift charts for binary data\n","- Plot the ROC (combine all measures in one graph)\n","\n","### Confusion Matrix and the Accuracy Rate "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Confusion Matrix and acccuracy for Logistic Regression:\")\n","classificationSummary(logit_result.actual, logit_result.predicted)\n","\n","print(\"-----------------------------------------------------------\")\n","print(\"Confusion Matrix and acccuracy for LogReg reduced:\")\n","classificationSummary(newlogit_result.actual, newlogit_result.predicted)\n","print(\"\")\n","print(\"-----------------------------------------------------------\")\n","print(\"Confusion Matrix and acccuracy for LogReg reduced:\")\n","classificationSummary(fullClassTree_result.actual, fullClassTree_result.predicted)\n","print(\"\")\n","print(\"-----------------------------------------------------------\")\n","print(\"Acccuracy for ANN model:\")\n","\n","history.history['val_accuracy'][-1]"]},{"cell_type":"markdown","metadata":{},"source":["### Gains and Lift charts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ax = gainsChart(logit_result.actual, label='Full model', color='C1', figsize=[5, 5])\n","ax = gainsChart(newlogit_result.actual, label='Reduced model', color='C0', ax=ax)\n","ax = gainsChart(fullClassTree_result.actual, label='Classification model', color='C2', ax=ax)\n","ax.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","fig, axs = plt.subplots(1, 3, figsize = (10, 10))\n","\n","liftChart(logit_result['p(1)'], title='Full Model', labelBars=False, ax=axs[0])\n","liftChart(newlogit_result['p(1)'], title='Reduced model', labelBars=False, ax=axs[1])\n","liftChart(fullClassTree_result['p(1)'], title='Classification model', labelBars=False, ax=axs[2])\n","ax.legend()\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### ROC Curves & AUC"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_y1 = logit.predict_proba(X_test)[:,1]  #Logistic Regression\n","pred_y2 = newlogit.predict_proba(X_test.drop(var_to_del, axis=1))[:,1] # Log Reg w/ Reduced variables\n","pred_y3 = fullClassTree.predict_proba(X_test)[:,1] # Classification Regression\n","pred_y4 = annmodel.predict(X_test_ann) # keras ANN\n","\n","fpr1, tpr1, threshold1 = roc_curve(y_test,pred_y1)\n","fpr2, tpr2, threshold2 = roc_curve(y_test,pred_y2)\n","fpr3, tpr3, threshold3 = roc_curve(y_test,pred_y3)\n","fpr4, tpr4, threshold4 = roc_curve(y_test,pred_y4)\n","\n","roc_auc1 = auc(fpr1, tpr1)\n","roc_auc2 = auc(fpr2, tpr2)\n","roc_auc3 = auc(fpr3, tpr3)\n","roc_auc4 = auc(fpr4, tpr4)\n","\n","plt.figure(figsize=(7,7))\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr1, tpr1, 'b', label = 'AUC Sklearn Logit= %0.2f' % roc_auc1)\n","plt.plot(fpr2, tpr2, 'y', label = 'AUC Sklearn Log Red= %0.2f' % roc_auc2)\n","plt.plot(fpr3, tpr3, 'r', label = 'AUC Classification= %0.2f' % roc_auc3)\n","plt.plot(fpr4, tpr4, 'g', label = 'AUC Keras-ANN= %0.2f' % roc_auc4)\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show()"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"c7575edc-74cd-4490-894d-9976f305c493","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"}},"nbformat":4,"nbformat_minor":4}